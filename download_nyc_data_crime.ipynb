{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3a9f4e",
   "metadata": {},
   "source": [
    "# NYC Open Data Downloader\n",
    "\n",
    "This notebook downloads 911 data from NYC Open Data API. It retrieves records for the year 2024 with specific radio codes, handling the data in batches to manage memory efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9464b",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "We'll need the following libraries:\n",
    "- `requests`: For making HTTP requests\n",
    "- `json`: For handling JSON data\n",
    "- `gzip`: For compression\n",
    "- `datetime`: For timestamp handling\n",
    "- `pandas`: For data manipulation\n",
    "- `time`: For adding delays between requests\n",
    "- `dotenv`: For loading environment variables from .env file\n",
    "- `os`: For environment variables\n",
    "- `pathlib`: For handling file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22bd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec054a",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "Set up the API endpoint and authentication headers using credentials from a .env file. We'll also define our pagination parameters.\n",
    "\n",
    "Note: Make sure you have a .env file in the same directory with your `NYC_DATA_AUTH` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc4cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.cityofnewyork.us/api/v3/views/d6zx-ckhd/query.json'\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_path = Path().absolute() / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Get authorization from .env file\n",
    "auth_token = os.getenv('NYC_DATA_AUTH')\n",
    "if not auth_token:\n",
    "    raise ValueError(\".env file missing NYC_DATA_AUTH value\")\n",
    "\n",
    "HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Basic {auth_token}'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "page_number = 1\n",
    "page_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72efd0",
   "metadata": {},
   "source": [
    "## Data Download Loop\n",
    "This section contains the main loop that:\n",
    "1. Constructs the query payload\n",
    "2. Makes API requests\n",
    "3. Handles pagination\n",
    "4. Saves intermediate results\n",
    "5. Implements error handling and retries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfb727",
   "metadata": {},
   "source": [
    "We will first run the loop to extract the data for radio code that correspond to criminal activites (//to do attach link to file with mapping of radio code to categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6ee063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1... Timestamp: 2025-11-01 19:57:11\n",
      "Retrieved 50000 records. Total so far: 50000\n",
      "Fetching page 2... Timestamp: 2025-11-01 19:57:21\n",
      "Retrieved 50000 records. Total so far: 100000\n",
      "Fetching page 3... Timestamp: 2025-11-01 19:58:38\n",
      "Retrieved 50000 records. Total so far: 150000\n",
      "Fetching page 4... Timestamp: 2025-11-01 19:58:45\n",
      "Retrieved 50000 records. Total so far: 200000\n",
      "Fetching page 5... Timestamp: 2025-11-01 19:58:53\n",
      "Retrieved 50000 records. Total so far: 250000\n",
      "Fetching page 6... Timestamp: 2025-11-01 19:59:00\n",
      "Retrieved 50000 records. Total so far: 300000\n",
      "Fetching page 7... Timestamp: 2025-11-01 19:59:06\n",
      "Retrieved 50000 records. Total so far: 350000\n",
      "Fetching page 8... Timestamp: 2025-11-01 19:59:13\n",
      "Retrieved 50000 records. Total so far: 400000\n",
      "Fetching page 9... Timestamp: 2025-11-01 19:59:19\n",
      "Retrieved 50000 records. Total so far: 450000\n",
      "Fetching page 10... Timestamp: 2025-11-01 19:59:26\n",
      "Retrieved 50000 records. Total so far: 500000\n",
      "Fetching page 11... Timestamp: 2025-11-01 19:59:33\n",
      "Retrieved 50000 records. Total so far: 550000\n",
      "Fetching page 12... Timestamp: 2025-11-01 19:59:38\n",
      "Retrieved 50000 records. Total so far: 600000\n",
      "Fetching page 13... Timestamp: 2025-11-01 19:59:44\n",
      "Retrieved 50000 records. Total so far: 650000\n",
      "Fetching page 14... Timestamp: 2025-11-01 19:59:50\n",
      "Retrieved 50000 records. Total so far: 700000\n",
      "Fetching page 15... Timestamp: 2025-11-01 19:59:57\n",
      "Retrieved 50000 records. Total so far: 750000\n",
      "Fetching page 16... Timestamp: 2025-11-01 20:00:05\n",
      "Retrieved 50000 records. Total so far: 800000\n",
      "Fetching page 17... Timestamp: 2025-11-01 20:00:13\n",
      "Retrieved 15867 records. Total so far: 815867\n",
      "Reached the last page.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    payload = {\n",
    "        \"query\": \"SELECT cad_evnt_id, create_date,incident_date,incident_time,nypd_pct_cd,boro_nm,patrl_boro_nm,geo_cd_x,geo_cd_y,radio_code,typ_desc,cip_jobs, add_ts, arrivd_ts, disp_ts,closng_ts,latitude,longitude  WHERE incident_date between '2024-01-01' and '2024-12-31' AND boro_nm IS NOT NULL AND patrl_boro_nm IS NOT NULL AND nypd_pct_cd IS NOT NULL AND closng_ts IS NOT NULL AND radio_code IS NOT NULL AND typ_desc IS NOT NULL AND cip_jobs IS NOT NULL AND disp_ts IS NOT NULL AND arrivd_ts IS NOT NULL AND radio_code in ('34K7', '34K6', '34K1', '34K8', '34K2', '34K5', '34K9', '34Q7', '34Q6', '34Q1', '34Q8', '34Q2', '34Q5', '34Q9', '34S7', '34S6', '34S1', '34S8', '34S2', '34S5', '34S9', '34W7', '34W6', '34W1', '34W8', '34W2', '34W5', '34W9', '24K7', '24K6', '24K1', '24K8', '24K2', '24K5', '24K9', '24Q7', '24Q6', '24Q1', '24Q8', '24Q2', '24Q5', '24Q9', '24S7', '24S6', '24S1', '24S2', '24S5', '24S9', '24W7', '24W6', '24W1', '24W8', '24W2', '24W5', '24W9', '31C', '31C9', '31Q', '31C8', '31Q8', '31Q5', '31Q9', '31R', '21C', '21C9', '21Q', '21Q5', '21Q9', '21R', '50G1', '50G8', '50G2', '50G5', '50G9', '50N1', '50N8', '50N2', '50N9', '50P1', '50P8', '50P2', '50P5', '50P9', '32P1', '32P8', '32P2', '32P5', '32P9', '32Q1', '32Q8', '32Q2', '32Q5', '32Q9', '32V1', '32V8', '32V2', '32V9', '22P1', '22P8', '22P2', '22P5', '22P9', '22Q1', '22Q8', '22Q2', '22Q5', '22Q9', '22V1', '22V8', '22V2', '22V5', '22V9', '39C1', '39C8', '39C2', '39C5', '39C9', '39P6', '39G8', '39G9', '39G', '39H1', '39H8', '39H2', '39H5', '39H9', '39Q1', '39Q8', '39Q2', '39Q5', '39Q9', '39P', '39T1', '39T8', '39T2', '39T5', '39T9', '39V6', '39V1', '39V8', '39V2', '39V5', '39V9', '29C8', '29C9', '29C', '29B9', '29H1', '29H8', '29H2', '29H5', '29H9', '29Q1', '29Q8', '29Q2', '29Q5', '29Q9', '29T1', '29T2', '29T5', '29T9', '69M1', '69M2', '69M9', '30B', '30C', '30C8', '30C9', '30Q1', '30Q8', '30Q2', '30Q5', '30Q9', '30R', '20B', '20C', '20Q1', '20Q8', '20Q2', '20Q5', '20Q9', '20R')\",\n",
    "        \"page\": {\n",
    "            \"pageNumber\": page_number,\n",
    "            \"pageSize\": page_size\n",
    "        },\n",
    "        \"includeSynthetic\": False\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching page {page_number}... Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=payload, timeout=300)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout error on page {page_number}. Retrying...\")\n",
    "        sleep(5)\n",
    "        continue\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error on page {page_number}: {e}\")\n",
    "        break\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if we got any data\n",
    "    if not data or len(data) == 0:\n",
    "        print(\"No more data to fetch.\")\n",
    "        break\n",
    "    \n",
    "    all_data.extend(data)\n",
    "    print(f\"Retrieved {len(data)} records. Total so far: {len(all_data)}\")\n",
    "    \n",
    "    # If we got fewer records than page_size, we've reached the end\n",
    "    if len(data) < page_size:\n",
    "        print(\"Reached the last page.\")\n",
    "        break\n",
    "    \n",
    "    page_number += 1\n",
    "    sleep(1)  # Add a 1-second delay between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa8f53",
   "metadata": {},
   "source": [
    "## Save Final Results\n",
    "Save the complete dataset to a compressed JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b664fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records extracted: 815867\n",
      "Data saved to nyc_data_crime.json.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal records extracted: {len(all_data)}\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_filename = 'nyc_data_crime.json.gz'\n",
    "with gzip.open(output_filename, 'wt', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, separators=(',', ':'))\n",
    "\n",
    "print(f\"Data saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fa26a",
   "metadata": {},
   "source": [
    "Save the complete dataset to a compressed parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce42917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to nyc_data_crime.parquet\n",
      "Shape: (815867, 18)\n",
      "Columns: ['cad_evnt_id', 'create_date', 'incident_date', 'incident_time', 'nypd_pct_cd', 'boro_nm', 'patrl_boro_nm', 'geo_cd_x', 'geo_cd_y', 'radio_code', 'typ_desc', 'cip_jobs', 'add_ts', 'arrivd_ts', 'disp_ts', 'closng_ts', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save as Parquet with zstd compression\n",
    "output_filename = 'nyc_data_crime.parquet'\n",
    "df.to_parquet(output_filename, compression='zstd', index=False)\n",
    "\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c4845",
   "metadata": {},
   "source": [
    "Now downloading data for potential-criminal activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f255e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1... Timestamp: 2025-11-01 20:08:23\n",
      "Retrieved 50000 records. Total so far: 50000\n",
      "Fetching page 2... Timestamp: 2025-11-01 20:08:46\n",
      "Timeout error on page 2. Retrying...\n",
      "Fetching page 2... Timestamp: 2025-11-01 20:13:51\n",
      "Retrieved 50000 records. Total so far: 100000\n",
      "Fetching page 3... Timestamp: 2025-11-01 20:14:39\n",
      "Retrieved 50000 records. Total so far: 150000\n",
      "Fetching page 4... Timestamp: 2025-11-01 20:14:53\n",
      "Retrieved 50000 records. Total so far: 200000\n",
      "Fetching page 5... Timestamp: 2025-11-01 20:15:02\n",
      "Retrieved 50000 records. Total so far: 250000\n",
      "Fetching page 6... Timestamp: 2025-11-01 20:15:13\n",
      "Retrieved 50000 records. Total so far: 300000\n",
      "Fetching page 7... Timestamp: 2025-11-01 20:15:23\n",
      "Retrieved 50000 records. Total so far: 350000\n",
      "Fetching page 8... Timestamp: 2025-11-01 20:15:34\n",
      "Retrieved 50000 records. Total so far: 400000\n",
      "Fetching page 9... Timestamp: 2025-11-01 20:15:46\n",
      "Retrieved 50000 records. Total so far: 450000\n",
      "Fetching page 10... Timestamp: 2025-11-01 20:15:56\n",
      "Retrieved 50000 records. Total so far: 500000\n",
      "Fetching page 11... Timestamp: 2025-11-01 20:16:03\n",
      "Retrieved 50000 records. Total so far: 550000\n",
      "Fetching page 12... Timestamp: 2025-11-01 20:16:14\n",
      "Retrieved 50000 records. Total so far: 600000\n",
      "Fetching page 13... Timestamp: 2025-11-01 20:16:27\n",
      "Retrieved 50000 records. Total so far: 650000\n",
      "Fetching page 14... Timestamp: 2025-11-01 20:16:35\n",
      "Retrieved 50000 records. Total so far: 700000\n",
      "Fetching page 15... Timestamp: 2025-11-01 20:16:43\n",
      "Retrieved 50000 records. Total so far: 750000\n",
      "Fetching page 16... Timestamp: 2025-11-01 20:17:02\n",
      "Retrieved 50000 records. Total so far: 800000\n",
      "Fetching page 17... Timestamp: 2025-11-01 20:17:13\n",
      "Retrieved 50000 records. Total so far: 850000\n",
      "Fetching page 18... Timestamp: 2025-11-01 20:17:28\n",
      "Retrieved 50000 records. Total so far: 900000\n",
      "Fetching page 19... Timestamp: 2025-11-01 20:17:39\n",
      "Retrieved 50000 records. Total so far: 950000\n",
      "Fetching page 20... Timestamp: 2025-11-01 20:17:49\n",
      "Retrieved 50000 records. Total so far: 1000000\n",
      "Fetching page 21... Timestamp: 2025-11-01 20:18:01\n",
      "Retrieved 50000 records. Total so far: 1050000\n",
      "Fetching page 22... Timestamp: 2025-11-01 20:18:09\n",
      "Retrieved 50000 records. Total so far: 1100000\n",
      "Fetching page 23... Timestamp: 2025-11-01 20:18:20\n",
      "Retrieved 17591 records. Total so far: 1117591\n",
      "Reached the last page.\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "page_number = 1\n",
    "page_size = 50000\n",
    "while True:\n",
    "    payload = {\n",
    "        \"query\": \"SELECT cad_evnt_id, create_date,incident_date,incident_time,nypd_pct_cd,boro_nm,patrl_boro_nm,geo_cd_x,geo_cd_y,radio_code,typ_desc,cip_jobs, add_ts, arrivd_ts, disp_ts,closng_ts,latitude,longitude  WHERE incident_date between '2024-01-01' and '2024-12-31' AND boro_nm IS NOT NULL AND patrl_boro_nm IS NOT NULL AND nypd_pct_cd IS NOT NULL AND closng_ts IS NOT NULL AND radio_code IS NOT NULL AND typ_desc IS NOT NULL AND cip_jobs IS NOT NULL AND disp_ts IS NOT NULL AND arrivd_ts IS NOT NULL AND radio_code in ('11A1', '11A2', '11A5', '11A9', '11B4', '11B3', '11B9', '11C4', '11C3', '11C8', '11Q4', '11Q3', '11Q9', '11R3', '11R4', '52D6', '52F6', '52F1', '52F8', '52F2', '52F5', '52F9', '52D1', '52K6', '52K1', '52K8', '52K2', '52K5', '52K9', '52D8', '52D2', '52A6', '52D5', '52D9', '52V6', '52V1', '52V2', '52V5', '52V9', '33', '33S5', '33T9', '10H1', '10H8', '10H2', '10H5', '10H9', '10F1', '10F8', '10F2', '10F5', '10F9', '10K1', '10K8', '10K2', '10K5', '10K9', '10M1', '10M2', '10M5', '10M9', '10N1', '10N8', '10N2', '10N5', '10N9', '10Q1', '10Q8', '10Q2', '10Y5', '10Q5', '10Q9', '10Y7', '10Y8', '10Y9', '10Y3', '10S1', '10S2', '10S5', '10S9', '10S8', '10P1', '10P8', '10P2', '10P5', '10P9', '10V1', '10V8', '10V2', '10V9', '40V', '40V8', '40V5', '12Z1', '12Z2', '12U1', '12U2', '12U5', '12U9', '12X', '12X1', '12X8', '12X2', '12X5', '12X9', '71S', '10S4', '44L1', '44L2', '44L5', '44P1', '44P8', '44P2', '44P5', '44P9', '44S1', '44S2', '44S9')\",\n",
    "        \"page\": {\n",
    "            \"pageNumber\": page_number,\n",
    "            \"pageSize\": page_size\n",
    "        },\n",
    "        \"includeSynthetic\": False\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching page {page_number}... Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "    try:\n",
    "        response = requests.post(url, headers=HEADERS, json=payload, timeout=300)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout error on page {page_number}. Retrying...\")\n",
    "        sleep(5)\n",
    "        continue\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error on page {page_number}: {e}\")\n",
    "        break\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if we got any data\n",
    "    if not data or len(data) == 0:\n",
    "        print(\"No more data to fetch.\")\n",
    "        break\n",
    "    \n",
    "    all_data.extend(data)\n",
    "    print(f\"Retrieved {len(data)} records. Total so far: {len(all_data)}\")\n",
    "    \n",
    "    # If we got fewer records than page_size, we've reached the end\n",
    "    if len(data) < page_size:\n",
    "        print(\"Reached the last page.\")\n",
    "        break\n",
    "    \n",
    "    page_number += 1\n",
    "    sleep(1)  # Add a 1-second delay between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6925c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records extracted: 1117591\n",
      "Data saved to nyc_data_potential_crime.json.gz\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal records extracted: {len(all_data)}\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_filename = 'nyc_data_potential_crime.json.gz'\n",
    "with gzip.open(output_filename, 'wt', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, separators=(',', ':'))\n",
    "\n",
    "print(f\"Data saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614647fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to nyc_data_potential_crime.parquet\n",
      "Shape: (1117591, 18)\n",
      "Columns: ['cad_evnt_id', 'create_date', 'incident_date', 'incident_time', 'nypd_pct_cd', 'boro_nm', 'patrl_boro_nm', 'geo_cd_x', 'geo_cd_y', 'radio_code', 'typ_desc', 'cip_jobs', 'add_ts', 'arrivd_ts', 'disp_ts', 'closng_ts', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save as Parquet with zstd compression\n",
    "output_filename = 'nyc_data_potential_crime.parquet'\n",
    "df.to_parquet(output_filename, compression='zstd', index=False)\n",
    "\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
